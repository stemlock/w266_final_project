{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Base_Model_Colab_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stemlock/w266_final_project/blob/master/Base_Model_Colab_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cKiLnoGAJWu"
      },
      "source": [
        "This workbook follows the example here: https://huggingface.co/transformers/custom_datasets.html?highlight=sequence#seq-imdb\n",
        "\n",
        "Can download the data directly from Stanford website with the following two commands:\n",
        "wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TabsksoQAc_k"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/My Drive/W266 Final Project/Code'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTlSloE8FOKl"
      },
      "source": [
        "# This only needs to be run once to install the transformers library\n",
        "# import os, sys\n",
        "# nb_path = '/content/notebooks'\n",
        "# os.symlink('/content/drive/My Drive/', nb_path)\n",
        "# sys.path.insert(0,nb_path)\n",
        "\n",
        "# !pip install --target=$nb_path transformers\n",
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TlOLrfYAJWz"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8-9RJ5FAJW1"
      },
      "source": [
        "# Set random seed\n",
        "seed = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD-Lsa0qAJW1"
      },
      "source": [
        "# # # Preprocess data in script to create neutral and gendered versions\n",
        "# !python3 preprocess.py -d 'data/aclImdb/train/' -v 'wordlist/' -o 'data/processed_train.csv'\n",
        "# !python3 preprocess.py -d 'data/aclImdb/test/' -v 'wordlist/' -o 'data/processed_test.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7n1eWvPAJW3"
      },
      "source": [
        "# Read in processed data (Rows with NA in the neutral_review_text had no tokens replaced)\n",
        "df_train = pd.read_csv('data/processed_train.csv')\n",
        "df_test = pd.read_csv('data/processed_test.csv')\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag_m8fkqAJW3"
      },
      "source": [
        "# Check how many reviews had no replacement tokens\n",
        "print(\"Nongendered revies in train:\", df_train['neutral_review_text'].isna().sum())\n",
        "print(\"Nongendered revies in test:\", df_test['neutral_review_text'].isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM2Hxm8_AJW4"
      },
      "source": [
        "# Isolate the nongendered reviews \n",
        "df_nongendered = df_train[df_train['neutral_review_text'].isna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fva2KgOAJW4"
      },
      "source": [
        "# Check the balance of review scores in the original train set\n",
        "df_train['review_score'].value_counts()/len(df_train['review_score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeUb4vRPAJW5"
      },
      "source": [
        "# Check the balance of review scores in the nongendered reviews\n",
        "df_nongendered['review_score'].value_counts()/len(df_nongendered['review_score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRIC3wkAJW6"
      },
      "source": [
        "# Check the distribution of the review scores in train\n",
        "df_train.hist(column='review_score')\n",
        "plt.title(\"Histogram of review scores in train dataset\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeAMSGpcAJW6"
      },
      "source": [
        "# Check the distribution of the review scores in nongendered reviews\n",
        "df_nongendered.hist(column='review_score')\n",
        "plt.title(\"Histogram of review scores in nongendered subset\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI1oN8vAAJW6"
      },
      "source": [
        "# Drop all nongendered rows in train by checking the neutral_review_text column\n",
        "df_train.dropna(inplace=True)\n",
        "print(\"Number of rows left in train:\", df_train.shape[0])\n",
        "print(\"Class balance:\")\n",
        "print(df_train['label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NZNTP4OAJW7"
      },
      "source": [
        "# Drop all nongendered rows in test by checking the neutral_review_text column\n",
        "df_test.dropna(inplace=True)\n",
        "print(\"Number of rows left in test:\", df_test.shape[0])\n",
        "print(\"Class balance:\")\n",
        "print(df_test['label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I9lXGmQyN4T"
      },
      "source": [
        "# ## TO DO: Try to fix this function. For some reason, this causes the model.fit() to fail\n",
        "# def encode_datasets(X_train, y_train, X_test, y_test, tokenizer, split_size=0.5, seed=42):\n",
        "\n",
        "#   '''\n",
        "#   Takes in train and test data and encodes them into train, dev, and test\n",
        "#   TF datasets using the provided tokenizer.\n",
        "#   '''\n",
        "\n",
        "#   # Split test set into dev and test\n",
        "#   X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=split_size, random_state=seed)\n",
        "\n",
        "#   # Apply tokenizer to each dataset\n",
        "#   train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
        "#   dev_encodings = tokenizer(X_dev, truncation=True, padding=True)\n",
        "#   test_encodings = tokenizer(X_test, truncation=True, padding=True)\n",
        "\n",
        "#   # Turn encodings into datasets for easy batching\n",
        "#   train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "#       dict(train_encodings),\n",
        "#       y_train\n",
        "#   ))\n",
        "#   dev_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "#       dict(dev_encodings),\n",
        "#       y_dev\n",
        "#   ))\n",
        "#   test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "#       dict(test_encodings),\n",
        "#       y_test\n",
        "#   ))\n",
        "\n",
        "#   return train_dataset, dev_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU9Qx8R5-NjG"
      },
      "source": [
        "# # Load data\n",
        "# train_texts = df_train['review_text'].values.tolist()\n",
        "# n_train_texts = df_train['neutral_review_text'].values.tolist()\n",
        "# f_train_texts = df_train['female_review_text'].values.tolist()\n",
        "# m_train_texts = df_train['male_review_text'].values.tolist()\n",
        "# train_labels = df_train['label'].values.tolist()\n",
        "\n",
        "# test_texts = df_test['review_text'].values.tolist()\n",
        "# n_test_texts = df_test['neutral_review_text'].values.tolist()\n",
        "# f_test_texts = df_test['female_review_text'].values.tolist()\n",
        "# m_test_texts = df_test['male_review_text'].values.tolist()\n",
        "# test_labels = df_test['label'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHS5FQKcz2Th"
      },
      "source": [
        "# # Specify tokenizer and encode each dataset\n",
        "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# train_dataset, dev_dataset, test_dataset = encode_datasets(train_texts, train_labels, \n",
        "#                                                            test_texts, test_labels,\n",
        "#                                                            tokenizer)\n",
        "# n_train_dataset, n_dev_dataset, n_test_dataset = encode_datasets(n_train_texts, train_labels, \n",
        "#                                                                  n_test_texts, test_labels,\n",
        "#                                                                  tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8i696DrAJW7"
      },
      "source": [
        "# Load data\n",
        "train_texts = df_train['review_text'].values.tolist()\n",
        "n_train_texts = df_train['neutral_review_text'].values.tolist()\n",
        "f_train_texts = df_train['female_review_text'].values.tolist()\n",
        "m_train_texts = df_train['male_review_text'].values.tolist()\n",
        "train_labels = df_train['label'].values.tolist()\n",
        "\n",
        "test_texts = df_test['review_text'].values.tolist()\n",
        "n_test_texts = df_test['neutral_review_text'].values.tolist()\n",
        "f_test_texts = df_test['female_review_text'].values.tolist()\n",
        "m_test_texts = df_test['male_review_text'].values.tolist()\n",
        "test_labels = df_test['label'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt-BL3PBAJW8"
      },
      "source": [
        "# Create dev set from portion of train set\n",
        "dev_texts, test_texts, _, _ = train_test_split(test_texts, test_labels, test_size=.5, random_state=seed)\n",
        "n_dev_texts, n_test_texts, _, _ = train_test_split(n_test_texts, test_labels, test_size=.5, random_state=seed)\n",
        "f_dev_texts, f_test_texts, _, _ = train_test_split(f_test_texts, test_labels, test_size=.5, random_state=seed)\n",
        "m_dev_texts, m_test_texts, dev_labels, test_labels = train_test_split(m_test_texts, test_labels, \n",
        "                                                                        test_size=.5, random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VVafMpfAJW8"
      },
      "source": [
        "# Specify tokenizer and apply to each dataset\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "dev_encodings = tokenizer(dev_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXYKPoMoAJW8"
      },
      "source": [
        "# Turn encodings into datasets for easy batching\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "dev_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(dev_encodings),\n",
        "    dev_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgfwqs5i89wy"
      },
      "source": [
        "# Specify tokenizer and apply to each dataset\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "n_train_encodings = tokenizer(n_train_texts, truncation=True, padding=True)\n",
        "n_dev_encodings = tokenizer(n_dev_texts, truncation=True, padding=True)\n",
        "n_test_encodings = tokenizer(n_test_texts, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ois9IdKx9MtH"
      },
      "source": [
        "# Turn encodings into datasets for easy batching\n",
        "n_train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(n_train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "n_dev_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(n_dev_encodings),\n",
        "    dev_labels\n",
        "))\n",
        "n_test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(n_test_encodings),\n",
        "    test_labels\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbOfz02TYJJk"
      },
      "source": [
        "# Initiliaze the TPU devices\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices:\")\n",
        "print(tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "# Create the distribution strategy for training on TPU cluster\n",
        "tpu_strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADGep-yFc28D"
      },
      "source": [
        "1-sum(dev_labels)/len(dev_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip8Ox5RiAjKn"
      },
      "source": [
        "# Create the model within each device scope\n",
        "histories1 = []\n",
        "for train, dev in [(train_dataset, dev_dataset), (n_train_dataset, n_dev_dataset)]:\n",
        "  with tpu_strategy.scope():\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    \n",
        "    # model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.Accuracy(), \n",
        "    #                                                                         tf.keras.metrics.Precision(), \n",
        "    #                                                                         tf.keras.metrics.Recall()])\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "    print(model.summary())\n",
        "\n",
        "  history = model.fit(train.batch(16).prefetch(1), validation_data=dev.batch(16).prefetch(1), epochs=10, batch_size=16, shuffle=True)\n",
        "  histories1.append(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46jT-IlMAJW9"
      },
      "source": [
        "model.save_model(\"original_base_model\")\n",
        "tokenizer.save_pretrained(\"original_tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}